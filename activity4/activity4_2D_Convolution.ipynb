{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 VGG16 Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 model from tensorflow.keras\n",
    "model = VGG16()\n",
    "# model detail\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve kernel weights from the 1st Convolutional layer\n",
    "kernels, biases = model.layers[1].get_weights()\n",
    "# View CNN layer 1 architecture\n",
    "model.layers[1].get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Image using keras and numpy\n",
    "# convert the image to an array\n",
    "img = img_to_array(img)\n",
    "# expand dimensions so that it represents a single 'sample'\n",
    "# -> reshape 3D(H, W, Ch) image to 4D image (sample, H, W, Ch)\n",
    "img = expand_dims(img, axis = 0)\n",
    "# prepare the image (e.g. scale pixel values for the vgg)\n",
    "img_ready = preprocess_input(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Model CNN Layer 1\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Results from CNN Layer 1 called feature map (shape = (sample = 1, 224, 224, n_filters))\n",
    "# CNN Layer 1 -> n_filters = 64\n",
    "feature_maps = model.predict(img_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images of feature_maps\n",
    "# Do Subplot() 8x8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Image Preparation (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reshape from 3D image (H, W, Ch) -> 4D image (1, H, W, Ch)\n",
    "\n",
    "# Do\n",
    "# Read Image\n",
    "# img.reshape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image resize (H, W) -> (224, 224)\n",
    "# Do\n",
    "# Becareful aspect ratio\n",
    "# cv2.resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image subtract dataset mean of R, G, B\n",
    "img_mean = [123.68, 116.779, 103.939] # -> meanR, meanG, meanB\n",
    "# image - img_mean do with mean each channel R,G,B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color conversion\n",
    "# RGB -> BGR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Cov2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate 2D convolution to image from 4.2 (imgBGR)\n",
    "    # image convolution with kernel each color channel (every color channel)\n",
    "img_result[:,:,0] = signal.convolve2d(imgBGR[:, :, 0], kernels[:,:,0,i], mode='same', boundary='fill', fillvalue=0)\n",
    "# -> zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum image convolutional result of B, G, R\n",
    "image_sum = img_result[:, :, B] + img_result[:, :, G] + img_result[:, : ,R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function\n",
    "# if image_sum < 0 -> change to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images of feature_maps\n",
    "# Subplot() 8x8 images"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
